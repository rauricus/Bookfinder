{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bbdfed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "NOTEBOOKS_DIR = os.getcwd()\n",
    "HOME_DIR = os.path.dirname(NOTEBOOKS_DIR)\n",
    "\n",
    "def get_next_directory(base_path=\"output/predict\"):\n",
    "    # Check if the base directory exists\n",
    "    if not os.path.exists(base_path):\n",
    "        return base_path\n",
    "    else:\n",
    "        # Find the next available numbered directory\n",
    "        i = 2\n",
    "        while os.path.exists(f\"{base_path}{i}\"):\n",
    "            i += 1\n",
    "        return f\"{base_path}{i}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f738e69-8ca5-46a7-8a8c-abeff29fa2ce",
   "metadata": {},
   "source": [
    "# Perform object detection\n",
    "\n",
    "Perform the inference with the loaded model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81faa6e6-cf3a-433e-a847-229f1c4bf69f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /Users/andreas/Documents/Projekte/Objekterkennung.yolo11/example-files/books/books_00005.png: 480x640 226.7ms\n",
      "Speed: 2.0ms preprocess, 226.7ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "#model = YOLO(\"yolo11s.pt\", )  # load an official model\n",
    "# model = YOLO(\"yolo11s-seg.pt\")  # load an official model (instance segmentation)\n",
    "model = YOLO(HOME_DIR+\"/runs/obb/train/weights/best.pt\")  # load an official model (Oriented Bounding Boxes Object Detection)\n",
    "#model = YOLO(HOME_DIR+\"/runs/segment/train/weights/best.pt\")  # load my custom model\n",
    "\n",
    "# source = 'https://ultralytics.com/images/bus.jpg'\n",
    "# source = HOME_DIR+'/example-files/IMG_3688.png'\n",
    "# source = HOME_DIR+'/example-files/books'\n",
    "# source = HOME_DIR+'/example-files/books.mov'\n",
    "source = HOME_DIR+'/example-files/books/books_00005.png'\n",
    "\n",
    "# Predict with the model\n",
    "results = model.predict(source, conf=0.5)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153d223f",
   "metadata": {},
   "source": [
    "# Adjust images (OBB only)\n",
    "\n",
    "From: https://github.com/ultralytics/ultralytics/issues/9344\n",
    "\n",
    "Switching to an OBB (oriented bounding box) model means you'll be working with rotated bounding boxes. The .boxes attribute for an OBB model will contain the center coordinates, width, height, and angle in radians.\n",
    "\n",
    "I took the following crop_rect function from here, as it also rotates the image: https://github.com/ultralytics/ultralytics/issues/13650"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa95859c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def crop_rect(img, rect, interpolation=cv2.INTER_CUBIC):\n",
    "    \"\"\"\n",
    "    Extracts and rectifies a rotated bounding box from an image.\n",
    "\n",
    "    This function takes an image and an oriented bounding box (OBB), rotates the \n",
    "    image such that the bounding box becomes axis-aligned (rectangular), and then \n",
    "    crops the bounding box area.\n",
    "\n",
    "    Args:\n",
    "        img (numpy.ndarray): The input image from which the bounding box is extracted.\n",
    "        rect (tuple): The oriented bounding box parameters.\n",
    "                     - rect[0]: Center coordinates of the bounding box (x, y).\n",
    "                     - rect[1]: Size of the bounding box (width, height).\n",
    "                     - rect[2]: Rotation angle of the bounding box (in degrees).\n",
    "        interpolation (int, optional): Interpolation method used when rotating the image.\n",
    "                                       Defaults to cv2.INTER_CUBIC.\n",
    "\n",
    "    Returns:\n",
    "        cropped_image (numpy.ndarray): The cropped rectangle region from the rotated image.\n",
    "    \"\"\"\n",
    "\n",
    "    # Process:\n",
    "    #    1. Extracts the center, size, and angle of the bounding box.\n",
    "    #    2. Computes a rotation matrix to align the bounding box with the image axes.\n",
    "    #    3. Rotates the image based on the calculated rotation matrix.\n",
    "    #    4. Crops the now axis-aligned bounding box from the rotated image.\n",
    "\n",
    "\n",
    "    # get the parameter of the small rectangle\n",
    "    center, size, angle = rect[0], rect[1], rect[2]\n",
    "    center, size = tuple(map(int, center)), tuple(map(int, size))\n",
    "\n",
    "    # get row and col num in img\n",
    "    height, width = img.shape[0], img.shape[1]\n",
    "\n",
    "    # calculate the rotation matrix\n",
    "    M = cv2.getRotationMatrix2D(center, angle, 1)\n",
    "    # rotate the original image\n",
    "    img_rot = cv2.warpAffine(img, M, (width, height), flags=interpolation)\n",
    "\n",
    "    # now rotated rectangle becomes vertical, and we crop it\n",
    "    img_crop = cv2.getRectSubPix(img_rot, size, center)\n",
    "\n",
    "    return img_crop\n",
    "\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def prepare_for_ocr(img):\n",
    "    \"\"\"\n",
    "    Processes a cropped rectangle for OCR detection by ensuring the image is wider than tall\n",
    "    and generating both the original (or rotated) and a 180-degree rotated variant.\n",
    "    \n",
    "    Args:\n",
    "        img (numpy.ndarray): The cropped rectangle image.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (processed_img, rotated_180_img)\n",
    "               - processed_img: Image oriented to be wider than tall.\n",
    "               - rotated_180_img: 180-degree rotated version of `processed_img`.\n",
    "    \"\"\"\n",
    "    # Get image dimensions\n",
    "    height, width = img.shape[:2]\n",
    "\n",
    "    # Rotate 90 degrees clockwise if the image is taller than it is wide\n",
    "    if height > width:\n",
    "        img = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\n",
    "\n",
    "    # Generate the 180-degree rotated image\n",
    "    rotated_180_img = cv2.rotate(img, cv2.ROTATE_180)\n",
    "\n",
    "    return img, rotated_180_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bff58f",
   "metadata": {},
   "source": [
    "# Save results, rotate and crop images (OBB only)\n",
    "\n",
    "The main code is again from https://github.com/ultralytics/ultralytics/issues/13650"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a9f9c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'book', 'class': 0, 'confidence': 0.92829, 'box': {'x1': 2138.47437, 'y1': 2193.13525, 'x2': 2474.677, 'y2': 2211.39307, 'x3': 2559.83325, 'y3': 643.29034, 'x4': 2223.63062, 'y4': 625.03278}}, {'name': 'book', 'class': 0, 'confidence': 0.9155, 'box': {'x1': 2761.04761, 'y1': 2158.79468, 'x2': 2935.44067, 'y2': 2165.02563, 'x3': 2985.83765, 'y3': 754.52094, 'x4': 2811.44458, 'y4': 748.28986}}, {'name': 'book', 'class': 0, 'confidence': 0.91296, 'box': {'x1': 2932.27344, 'y1': 2170.69312, 'x2': 3104.52686, 'y2': 2176.56909, 'x3': 3153.72607, 'y3': 734.31604, 'x4': 2981.47266, 'y4': 728.43994}}, {'name': 'book', 'class': 0, 'confidence': 0.90544, 'box': {'x1': 2501.48193, 'y1': 2174.44922, 'x2': 2753.79492, 'y2': 2187.65234, 'x3': 2831.42773, 'y3': 704.10999, 'x4': 2579.11475, 'y4': 690.90662}}, {'name': 'book', 'class': 0, 'confidence': 0.8734, 'box': {'x1': 3395.00635, 'y1': 2191.7168, 'x2': 3565.3833, 'y2': 2203.33594, 'x3': 3671.73682, 'y3': 643.80695, 'x4': 3501.35986, 'y4': 632.18793}}, {'name': 'book', 'class': 0, 'confidence': 0.84513, 'box': {'x1': 1702.9198, 'y1': 2186.4397, 'x2': 1848.88708, 'y2': 2191.30103, 'x3': 1898.17175, 'y3': 711.41461, 'x4': 1752.20447, 'y4': 706.55353}}, {'name': 'book', 'class': 0, 'confidence': 0.82285, 'box': {'x1': 1574.646, 'y1': 2192.63843, 'x2': 1693.94653, 'y2': 2194.91626, 'x3': 1723.32422, 'y3': 656.21381, 'x4': 1604.02368, 'y4': 653.9361}}, {'name': 'book', 'class': 0, 'confidence': 0.80939, 'box': {'x1': 3053.93799, 'y1': 2170.57959, 'x2': 3401.18311, 'y2': 2199.17871, 'x3': 3529.78809, 'y3': 637.69336, 'x4': 3182.54297, 'y4': 609.09412}}, {'name': 'book', 'class': 0, 'confidence': 0.79645, 'box': {'x1': 1333.94604, 'y1': 2134.27075, 'x2': 1490.21387, 'y2': 2132.92896, 'x3': 1479.5564, 'y3': 891.95294, 'x4': 1323.28857, 'y4': 893.29498}}, {'name': 'book', 'class': 0, 'confidence': 0.77945, 'box': {'x1': 1473.78955, 'y1': 2192.73584, 'x2': 1574.33911, 'y2': 2193.96631, 'x3': 1591.52271, 'y3': 789.70367, 'x4': 1490.97314, 'y4': 788.47333}}, {'name': 'book', 'class': 0, 'confidence': 0.74262, 'box': {'x1': 1157.23877, 'y1': 2151.72241, 'x2': 1272.46558, 'y2': 2148.18774, 'x3': 1233.41797, 'y3': 875.24426, 'x4': 1118.19116, 'y4': 878.77881}}, {'name': 'book', 'class': 0, 'confidence': 0.58797, 'box': {'x1': 1902.82422, 'y1': 2198.26636, 'x2': 2002.69678, 'y2': 2199.96509, 'x3': 2028.70679, 'y3': 670.87231, 'x4': 1928.83423, 'y4': 669.17346}}, {'name': 'book', 'class': 0, 'confidence': 0.56269, 'box': {'x1': 1710.62183, 'y1': 3022.11987, 'x2': 1736.32983, 'y2': 2637.97388, 'x3': 371.00467, 'y3': 2546.60327, 'x4': 345.29672, 'y4': 2930.74927}}]\n"
     ]
    }
   ],
   "source": [
    "# Create the output directory, if needed\n",
    "OUTPUT_DIR = get_next_directory(os.path.join(HOME_DIR, \"output/predict\"))\n",
    "os.makedirs(OUTPUT_DIR+\"/book\", exist_ok=True)\n",
    "\n",
    "# Get only filename with no directories and no extension\n",
    "filename = os.path.splitext(os.path.basename(source))[0]\n",
    "\n",
    "# Process results\n",
    "with open(OUTPUT_DIR+\"/results.json\", \"w\") as text_file:\n",
    "    for result in results:\n",
    "        \n",
    "        if (len(result) > 0):\n",
    "            result.show()\n",
    "\n",
    "            print(result.to_json(), file=text_file)\n",
    "\n",
    "            for idx, obb in enumerate(result.obb.xyxyxyxy):\n",
    "                points = obb.cpu().numpy().reshape((-1, 1, 2)).astype(int)\n",
    "                rect = cv2.minAreaRect(points)\n",
    "\n",
    "                # Rotate the image slightly so that it aligns with the axes.\n",
    "                img_cropped = crop_rect(result.orig_img, rect)\n",
    "\n",
    "                # Ensure the image is wider than tall and also return a variant rotated by 180 degrees.\n",
    "                img, img_rotated_180 = prepare_for_ocr(img_cropped)\n",
    "\n",
    "                cv2.imwrite(os.path.join(OUTPUT_DIR, \"book\", f\"{filename}_{idx}.jpg\"), img)\n",
    "                cv2.imwrite(os.path.join(OUTPUT_DIR, \"book\", f\"{filename}_rotated-180_{idx}.jpg\"), img_rotated_180)\n",
    "\n",
    "            result.save_txt(OUTPUT_DIR+\"/results.txt\", save_conf=True)\n",
    "\n",
    "            print(result.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7470ef06",
   "metadata": {},
   "source": [
    "# Perform OCR on each book image found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba21b051",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import pytesseract\n",
    "from pytesseract import Output\n",
    "\n",
    "\n",
    "def detect_text_east(image, east_model_path, min_confidence=0.5, nms_threshold=0.4):\n",
    "    \"\"\"\n",
    "    Detects text regions in an image using the EAST text detector with debugging.\n",
    "    \"\"\"\n",
    "    # Load the image and grab its dimensions\n",
    "    (H, W) = image.shape[:2]\n",
    "\n",
    "    # Define the EAST model input dimensions\n",
    "    newW, newH = 320, 320\n",
    "\n",
    "    # Resize the image to fit the EAST model input\n",
    "    resized_image = cv2.resize(image, (newW, newH))\n",
    "    (rH, rW) = (H / float(newH), W / float(newW))  # Scale factors\n",
    "\n",
    "    # Load the pre-trained EAST model\n",
    "    net = cv2.dnn.readNet(east_model_path)\n",
    "\n",
    "    # Prepare the input blob for the EAST model\n",
    "    blob = cv2.dnn.blobFromImage(resized_image, 1.0, (newW, newH),\n",
    "                                 (123.68, 116.78, 103.94), swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "\n",
    "    # Perform a forward pass to get scores and geometry\n",
    "    (scores, geometry) = net.forward([\"feature_fusion/Conv_7/Sigmoid\",\n",
    "                                      \"feature_fusion/concat_3\"])\n",
    "\n",
    "    # Decode predictions\n",
    "    [detections, confidences] = decodeBoundingBoxes(scores, geometry, min_confidence)\n",
    "\n",
    "    # Apply non-maxima suppression to eliminate overlapping boxes\n",
    "    indices = cv2.dnn.NMSBoxesRotated(detections, confidences, min_confidence, nms_threshold)\n",
    "\n",
    "    cropped_images = []\n",
    "\n",
    "    debug_image = image.copy()\n",
    "    \n",
    "    if len(indices) > 0:\n",
    "        for i in indices.flatten():\n",
    "            # Get 4 corners of the rotated rect\n",
    "            vertices = cv2.boxPoints(detections[i])\n",
    "\n",
    "            # Scale the bounding box coordinates based on the respective ratios\n",
    "            for j in range(4):\n",
    "                vertices[j][0] *= rW\n",
    "                vertices[j][1] *= rH\n",
    "\n",
    "            # Debug visualization of bounding boxes\n",
    "            verticesAsInt = np.array(vertices, dtype=np.int32)  # Konvertiere in Integer\n",
    "            cv2.polylines(debug_image, [verticesAsInt], isClosed=True, color=(0, 255, 0), thickness=2)\n",
    "\n",
    "            # Add the index label to the bounding box\n",
    "            center_x, center_y = np.mean(verticesAsInt, axis=0).astype(int)\n",
    "            cv2.putText(debug_image, str(i), (center_x, center_y),\n",
    "                        fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.5,\n",
    "                        color=(0, 0, 255), thickness=1)\n",
    "            \n",
    "            # Get cropped image using perspective transform\n",
    "            # cropped_image = fourPointsTransform(image, vertices)\n",
    "            # cropped_image = cv2.cvtColor(cropped_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            # cropped_images.append(cropped_image)\n",
    "\n",
    "\n",
    "    # Display the debug image with all bounding boxes\n",
    "    cv2.imshow(\"All Bounding Boxes\", debug_image)\n",
    "    cv2.waitKey(0)\n",
    "\n",
    "    return cropped_images\n",
    "\n",
    "\n",
    "def decodeBoundingBoxes(scores, geometry, scoreThresh):\n",
    "    detections = []\n",
    "    confidences = []\n",
    "\n",
    "    ############ CHECK DIMENSIONS AND SHAPES OF geometry AND scores ############\n",
    "    assert len(scores.shape) == 4, \"Incorrect dimensions of scores\"\n",
    "    assert len(geometry.shape) == 4, \"Incorrect dimensions of geometry\"\n",
    "    assert scores.shape[0] == 1, \"Invalid dimensions of scores\"\n",
    "    assert geometry.shape[0] == 1, \"Invalid dimensions of geometry\"\n",
    "    assert scores.shape[1] == 1, \"Invalid dimensions of scores\"\n",
    "    assert geometry.shape[1] == 5, \"Invalid dimensions of geometry\"\n",
    "    assert scores.shape[2] == geometry.shape[2], \"Invalid dimensions of scores and geometry\"\n",
    "    assert scores.shape[3] == geometry.shape[3], \"Invalid dimensions of scores and geometry\"\n",
    "    height = scores.shape[2]\n",
    "    width = scores.shape[3]\n",
    "    for y in range(0, height):\n",
    "\n",
    "        # Extract data from scores\n",
    "        scoresData = scores[0][0][y]\n",
    "        x0_data = geometry[0][0][y]\n",
    "        x1_data = geometry[0][1][y]\n",
    "        x2_data = geometry[0][2][y]\n",
    "        x3_data = geometry[0][3][y]\n",
    "        anglesData = geometry[0][4][y]\n",
    "        for x in range(0, width):\n",
    "            score = scoresData[x]\n",
    "\n",
    "            # If score is lower than threshold score, move to next x\n",
    "            if (score < scoreThresh):\n",
    "                continue\n",
    "\n",
    "            # Calculate offset\n",
    "            offsetX = x * 4.0\n",
    "            offsetY = y * 4.0\n",
    "            angle = anglesData[x]\n",
    "\n",
    "            # Calculate cos and sin of angle\n",
    "            cosA = np.cos(angle)\n",
    "            sinA = np.sin(angle)\n",
    "            h = x0_data[x] + x2_data[x]\n",
    "            w = x1_data[x] + x3_data[x]\n",
    "\n",
    "            # Calculate offset\n",
    "            offset = ([offsetX + cosA * x1_data[x] + sinA * x2_data[x], offsetY - sinA * x1_data[x] + cosA * x2_data[x]])\n",
    "\n",
    "            # Find points for rectangle\n",
    "            p1 = (-sinA * h + offset[0], -cosA * h + offset[1])\n",
    "            p3 = (-cosA * w + offset[0], sinA * w + offset[1])\n",
    "            center = (0.5 * (p1[0] + p3[0]), 0.5 * (p1[1] + p3[1]))\n",
    "            detections.append((center, (w, h), -1 * angle * 180.0 / np.pi))\n",
    "            confidences.append(float(score))\n",
    "\n",
    "    # Return detections and confidences\n",
    "    return [detections, confidences]\n",
    "\n",
    "\n",
    "def fourPointsTransform(frame, vertices):\n",
    "    vertices = np.asarray(vertices)\n",
    "    outputSize = (100, 32)\n",
    "    targetVertices = np.array([\n",
    "        [0, outputSize[1] - 1],\n",
    "        [0, 0],\n",
    "        [outputSize[0] - 1, 0],\n",
    "        [outputSize[0] - 1, outputSize[1] - 1]], dtype=\"float32\")\n",
    "\n",
    "    rotationMatrix = cv2.getPerspectiveTransform(vertices, targetVertices)\n",
    "    result = cv2.warpPerspective(frame, rotationMatrix, outputSize)\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def rotate_image(image, angle):\n",
    "    \"\"\"\n",
    "    Rotates an image by the specified angle without cropping.\n",
    "\n",
    "    Args:\n",
    "        image (numpy.ndarray): The input image.\n",
    "        angle (float): The angle to rotate.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The rotated image.\n",
    "    \"\"\"\n",
    "    (h, w) = image.shape[:2]\n",
    "    (cx, cy) = (w // 2, h // 2)\n",
    "\n",
    "    # Compute the rotation matrix\n",
    "    M = cv2.getRotationMatrix2D((cx, cy), angle, 1.0)\n",
    "    cos = np.abs(M[0, 0])\n",
    "    sin = np.abs(M[0, 1])\n",
    "\n",
    "    # Compute new bounding dimensions\n",
    "    new_w = int((h * sin) + (w * cos))\n",
    "    new_h = int((h * cos) + (w * sin))\n",
    "\n",
    "    # Adjust rotation matrix to account for translation\n",
    "    M[0, 2] += (new_w / 2) - cx\n",
    "    M[1, 2] += (new_h / 2) - cy\n",
    "\n",
    "    # Perform the rotation\n",
    "    return cv2.warpAffine(image, M, (new_w, new_h))\n",
    "\n",
    "\n",
    "def ocr_onImage(image_path):\n",
    "    \"\"\"\n",
    "    Perform OCR on an image, forcing horizontal text detection.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the image for OCR.\n",
    "\n",
    "    Returns:\n",
    "        str: The OCR-detected text.\n",
    "    \"\"\"\n",
    "\n",
    "    east_model_path = os.path.join(HOME_DIR, \"notebooks\", \"east_text_detection.pb\")\n",
    "\n",
    "    # Open image with Pillow to access DPI metadata\n",
    "    pil_image = Image.open(image_path)\n",
    "    dpi = pil_image.info.get(\"dpi\", (72, 72))  # Default to 72 DPI if not present\n",
    "\n",
    "    # Convert Pillow image to OpenCV format\n",
    "    image = np.array(pil_image)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # --- Detect text regions using EAST, correct orientation, and perform OCR. ---\n",
    "\n",
    "    # Detect text regions using EAST\n",
    "    cropped_images = detect_text_east(image, east_model_path)\n",
    "\n",
    "    # Load the image\n",
    "    ocr_results = {}\n",
    "\n",
    "    # Process each detected region\n",
    "    for i, imageWithText in enumerate(cropped_images):\n",
    "\n",
    "        cv2.imshow(f\"Region {i}\", imageWithText)\n",
    "        cv2.waitKey(0)\n",
    "\n",
    "        # Perform OCR on the corrected region\n",
    "        ocr_text = pytesseract.image_to_string(imageWithText, config=\"--psm 6\")\n",
    "        ocr_results[f\"text_region_{i}\"] = ocr_text.strip()\n",
    "\n",
    "    return ocr_results\n",
    "    \n",
    "\n",
    "# get grayscale image\n",
    "def grayscale(image):\n",
    "    return cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# noise removal\n",
    "def denoise(image):\n",
    "    return cv2.medianBlur(image, 5)\n",
    "\n",
    "# thresholding\n",
    "def thresholding(image):\n",
    "    return cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87c5f7f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book 0 found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-03 20:41:23.615 python[45636:356681] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-01-03 20:41:23.615 python[45636:356681] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/andreas/Documents/Projekte/Objekterkennung.yolo11/output/predict/book/books_00005_rotated-180_0.jpg ->\n",
      "Book 1 found\n",
      "/Users/andreas/Documents/Projekte/Objekterkennung.yolo11/output/predict/book/books_00005_rotated-180_1.jpg ->\n",
      "Book 2 found\n",
      "/Users/andreas/Documents/Projekte/Objekterkennung.yolo11/output/predict/book/books_00005_rotated-180_2.jpg ->\n",
      "Book 3 found\n",
      "/Users/andreas/Documents/Projekte/Objekterkennung.yolo11/output/predict/book/books_00005_rotated-180_3.jpg ->\n",
      "Book 4 found\n",
      "/Users/andreas/Documents/Projekte/Objekterkennung.yolo11/output/predict/book/books_00005_rotated-180_4.jpg ->\n",
      "Book 5 found\n",
      "/Users/andreas/Documents/Projekte/Objekterkennung.yolo11/output/predict/book/books_00005_rotated-180_5.jpg ->\n",
      "Book 6 found\n",
      "/Users/andreas/Documents/Projekte/Objekterkennung.yolo11/output/predict/book/books_00005_rotated-180_6.jpg ->\n",
      "Book 7 found\n",
      "/Users/andreas/Documents/Projekte/Objekterkennung.yolo11/output/predict/book/books_00005_rotated-180_7.jpg ->\n",
      "Book 8 found\n",
      "/Users/andreas/Documents/Projekte/Objekterkennung.yolo11/output/predict/book/books_00005_rotated-180_8.jpg ->\n",
      "Book 9 found\n",
      "/Users/andreas/Documents/Projekte/Objekterkennung.yolo11/output/predict/book/books_00005_rotated-180_9.jpg ->\n",
      "Book 10 found\n",
      "/Users/andreas/Documents/Projekte/Objekterkennung.yolo11/output/predict/book/books_00005_rotated-180_10.jpg ->\n",
      "Book 11 found\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'flatten'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m variant_filename \u001b[38;5;129;01min\u001b[39;00m image_variants:\n\u001b[1;32m     38\u001b[0m     img_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(OUTPUT_DIR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbook\u001b[39m\u001b[38;5;124m\"\u001b[39m, variant_filename)\n\u001b[0;32m---> 39\u001b[0m     detected_texts \u001b[38;5;241m=\u001b[39m \u001b[43mocr_onImage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Display OCR results\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ->\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 199\u001b[0m, in \u001b[0;36mocr_onImage\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m    194\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(image, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# --- Detect text regions using EAST, correct orientation, and perform OCR. ---\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# Detect text regions using EAST\u001b[39;00m\n\u001b[0;32m--> 199\u001b[0m cropped_images \u001b[38;5;241m=\u001b[39m \u001b[43mdetect_text_east\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meast_model_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m# Load the image\u001b[39;00m\n\u001b[1;32m    202\u001b[0m ocr_results \u001b[38;5;241m=\u001b[39m {}\n",
      "Cell \u001b[0;32mIn[5], line 43\u001b[0m, in \u001b[0;36mdetect_text_east\u001b[0;34m(image, east_model_path, min_confidence, nms_threshold)\u001b[0m\n\u001b[1;32m     40\u001b[0m indices \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mdnn\u001b[38;5;241m.\u001b[39mNMSBoxesRotated(detections, confidences, min_confidence, nms_threshold)\n\u001b[1;32m     42\u001b[0m cropped_images \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[43mindices\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m():\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m# Get 4 corners of the rotated rect\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     vertices \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mboxPoints(detections[i])\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;66;03m# Scale the bounding box coordinates based on the respective ratios\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'flatten'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "\n",
    "#import easyocr\n",
    "import pytesseract\n",
    "\n",
    "# --- Needs tesseract on the path. I've installed it via homebrew.\n",
    "\n",
    "# Get only filename with no directories and no extension\n",
    "filename = os.path.splitext(os.path.basename(source))[0]\n",
    "\n",
    "for result in results:\n",
    "\n",
    "    if (len(result) > 0):\n",
    "        # for object detection and instance separation\n",
    "        # i=1\n",
    "\n",
    "        # for OBB\n",
    "        i=0\n",
    "\n",
    "        for detection in result.summary(): \n",
    "            if (detection['name'] == 'book'):\n",
    "                print(f\"Book {i} found\")\n",
    "\n",
    "                # for object detection and instance separation\n",
    "                # image_filename = f\"image{i}.jpg\" if i>1 else 'image.jpg'\n",
    "                # image_path = OUTPUT_DIR + '/book/' + image_filename\n",
    "                \n",
    "                # for OBB\n",
    "                # Perform OCR on all (both) image variants.\n",
    "                image_variants = [\n",
    "                    f\"{filename}_{i}.jpg\",  # Original image\n",
    "                    f\"{filename}_rotated-180_{i}.jpg\"  # 180-degree rotated image\n",
    "                ]\n",
    "\n",
    "                # Iterate over each variant, process the OCR, and print the result\n",
    "                for variant_filename in image_variants:\n",
    "                    img_path = os.path.join(OUTPUT_DIR, \"book\", variant_filename)\n",
    "                    detected_texts = ocr_onImage(img_path)\n",
    "\n",
    "                # Display OCR results\n",
    "                print(f\"{img_path} ->\")\n",
    "                for region, text in detected_texts.items():\n",
    "                    print(f\"    {region}: {text}\")\n",
    "\n",
    "\n",
    "                i += 1\n",
    "            else:\n",
    "                print(\"Skipping\", detection['name'], '...')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (yolo11)",
   "language": "python",
   "name": "yolo11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
