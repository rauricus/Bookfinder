{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bbdfed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "NOTEBOOKS_DIR = os.getcwd()\n",
    "HOME_DIR = os.path.dirname(NOTEBOOKS_DIR)\n",
    "\n",
    "def get_next_directory(base_path=\"output/predict\"):\n",
    "    # Check if the base directory exists\n",
    "    if not os.path.exists(base_path):\n",
    "        return base_path\n",
    "    else:\n",
    "        # Find the next available numbered directory\n",
    "        i = 2\n",
    "        while os.path.exists(f\"{base_path}{i}\"):\n",
    "            i += 1\n",
    "        return f\"{base_path}{i}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f738e69-8ca5-46a7-8a8c-abeff29fa2ce",
   "metadata": {},
   "source": [
    "# Perform object detection\n",
    "\n",
    "Perform the inference with the loaded model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81faa6e6-cf3a-433e-a847-229f1c4bf69f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /Users/andreas/Documents/Projekte/Objekterkennung.yolo11/example-files/books/books_00005.png: 480x640 225.3ms\n",
      "Speed: 2.1ms preprocess, 225.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "#model = YOLO(\"yolo11s.pt\", )  # load an official model\n",
    "# model = YOLO(\"yolo11s-seg.pt\")  # load an official model (instance segmentation)\n",
    "model = YOLO(HOME_DIR+\"/runs/obb/train/weights/best.pt\")  # load an official model (Oriented Bounding Boxes Object Detection)\n",
    "#model = YOLO(HOME_DIR+\"/runs/segment/train/weights/best.pt\")  # load my custom model\n",
    "\n",
    "# source = 'https://ultralytics.com/images/bus.jpg'\n",
    "# source = HOME_DIR+'/example-files/IMG_3688.png'\n",
    "# source = HOME_DIR+'/example-files/books'\n",
    "# source = HOME_DIR+'/example-files/books.mov'\n",
    "source = HOME_DIR+'/example-files/books/books_00005.png'\n",
    "\n",
    "# Predict with the model\n",
    "results = model.predict(source, conf=0.5)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153d223f",
   "metadata": {},
   "source": [
    "# Adjust images (OBB only)\n",
    "\n",
    "From: https://github.com/ultralytics/ultralytics/issues/9344\n",
    "\n",
    "Switching to an OBB (oriented bounding box) model means you'll be working with rotated bounding boxes. The .boxes attribute for an OBB model will contain the center coordinates, width, height, and angle in radians.\n",
    "\n",
    "I took the following crop_rect function from here, as it also rotates the image: https://github.com/ultralytics/ultralytics/issues/13650"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa95859c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def crop_rect(img, rect, interpolation=cv2.INTER_CUBIC):\n",
    "    \"\"\"\n",
    "    Extracts and rectifies a rotated bounding box from an image.\n",
    "\n",
    "    This function takes an image and an oriented bounding box (OBB), rotates the \n",
    "    image such that the bounding box becomes axis-aligned (rectangular), and then \n",
    "    crops the bounding box area.\n",
    "\n",
    "    Args:\n",
    "        img (numpy.ndarray): The input image from which the bounding box is extracted.\n",
    "        rect (tuple): The oriented bounding box parameters.\n",
    "                     - rect[0]: Center coordinates of the bounding box (x, y).\n",
    "                     - rect[1]: Size of the bounding box (width, height).\n",
    "                     - rect[2]: Rotation angle of the bounding box (in degrees).\n",
    "        interpolation (int, optional): Interpolation method used when rotating the image.\n",
    "                                       Defaults to cv2.INTER_CUBIC.\n",
    "\n",
    "    Returns:\n",
    "        cropped_image (numpy.ndarray): The cropped rectangle region from the rotated image.\n",
    "    \"\"\"\n",
    "\n",
    "    # Process:\n",
    "    #    1. Extracts the center, size, and angle of the bounding box.\n",
    "    #    2. Computes a rotation matrix to align the bounding box with the image axes.\n",
    "    #    3. Rotates the image based on the calculated rotation matrix.\n",
    "    #    4. Crops the now axis-aligned bounding box from the rotated image.\n",
    "\n",
    "\n",
    "    # get the parameter of the small rectangle\n",
    "    center, size, angle = rect[0], rect[1], rect[2]\n",
    "    center, size = tuple(map(int, center)), tuple(map(int, size))\n",
    "\n",
    "    # get row and col num in img\n",
    "    height, width = img.shape[0], img.shape[1]\n",
    "\n",
    "    # calculate the rotation matrix\n",
    "    M = cv2.getRotationMatrix2D(center, angle, 1)\n",
    "    # rotate the original image\n",
    "    img_rot = cv2.warpAffine(img, M, (width, height), flags=interpolation)\n",
    "\n",
    "    # now rotated rectangle becomes vertical, and we crop it\n",
    "    img_crop = cv2.getRectSubPix(img_rot, size, center)\n",
    "\n",
    "    return img_crop\n",
    "\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def prepare_for_ocr(img):\n",
    "    \"\"\"\n",
    "    Processes a cropped rectangle for OCR detection by ensuring the image is wider than tall\n",
    "    and generating both the original (or rotated) and a 180-degree rotated variant.\n",
    "    \n",
    "    Args:\n",
    "        img (numpy.ndarray): The cropped rectangle image.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (processed_img, rotated_180_img)\n",
    "               - processed_img: Image oriented to be wider than tall.\n",
    "               - rotated_180_img: 180-degree rotated version of `processed_img`.\n",
    "    \"\"\"\n",
    "    # Get image dimensions\n",
    "    height, width = img.shape[:2]\n",
    "\n",
    "    # Rotate 90 degrees clockwise if the image is taller than it is wide\n",
    "    if height > width:\n",
    "        img = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\n",
    "\n",
    "    # Generate the 180-degree rotated image\n",
    "    rotated_180_img = cv2.rotate(img, cv2.ROTATE_180)\n",
    "\n",
    "    return img, rotated_180_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bff58f",
   "metadata": {},
   "source": [
    "# Save results, rotate and crop images (OBB only)\n",
    "\n",
    "The main code is again from https://github.com/ultralytics/ultralytics/issues/13650"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a9f9c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'book', 'class': 0, 'confidence': 0.92829, 'box': {'x1': 2138.47437, 'y1': 2193.13525, 'x2': 2474.677, 'y2': 2211.39307, 'x3': 2559.83325, 'y3': 643.29034, 'x4': 2223.63062, 'y4': 625.03278}}, {'name': 'book', 'class': 0, 'confidence': 0.9155, 'box': {'x1': 2761.04761, 'y1': 2158.79468, 'x2': 2935.44067, 'y2': 2165.02563, 'x3': 2985.83765, 'y3': 754.52094, 'x4': 2811.44458, 'y4': 748.28986}}, {'name': 'book', 'class': 0, 'confidence': 0.91296, 'box': {'x1': 2932.27344, 'y1': 2170.69312, 'x2': 3104.52686, 'y2': 2176.56909, 'x3': 3153.72607, 'y3': 734.31604, 'x4': 2981.47266, 'y4': 728.43994}}, {'name': 'book', 'class': 0, 'confidence': 0.90544, 'box': {'x1': 2501.48193, 'y1': 2174.44922, 'x2': 2753.79492, 'y2': 2187.65234, 'x3': 2831.42773, 'y3': 704.10999, 'x4': 2579.11475, 'y4': 690.90662}}, {'name': 'book', 'class': 0, 'confidence': 0.8734, 'box': {'x1': 3395.00635, 'y1': 2191.7168, 'x2': 3565.3833, 'y2': 2203.33594, 'x3': 3671.73682, 'y3': 643.80695, 'x4': 3501.35986, 'y4': 632.18793}}, {'name': 'book', 'class': 0, 'confidence': 0.84513, 'box': {'x1': 1702.9198, 'y1': 2186.4397, 'x2': 1848.88708, 'y2': 2191.30103, 'x3': 1898.17175, 'y3': 711.41461, 'x4': 1752.20447, 'y4': 706.55353}}, {'name': 'book', 'class': 0, 'confidence': 0.82285, 'box': {'x1': 1574.646, 'y1': 2192.63843, 'x2': 1693.94653, 'y2': 2194.91626, 'x3': 1723.32422, 'y3': 656.21381, 'x4': 1604.02368, 'y4': 653.9361}}, {'name': 'book', 'class': 0, 'confidence': 0.80939, 'box': {'x1': 3053.93799, 'y1': 2170.57959, 'x2': 3401.18311, 'y2': 2199.17871, 'x3': 3529.78809, 'y3': 637.69336, 'x4': 3182.54297, 'y4': 609.09412}}, {'name': 'book', 'class': 0, 'confidence': 0.79645, 'box': {'x1': 1333.94604, 'y1': 2134.27075, 'x2': 1490.21387, 'y2': 2132.92896, 'x3': 1479.5564, 'y3': 891.95294, 'x4': 1323.28857, 'y4': 893.29498}}, {'name': 'book', 'class': 0, 'confidence': 0.77945, 'box': {'x1': 1473.78955, 'y1': 2192.73584, 'x2': 1574.33911, 'y2': 2193.96631, 'x3': 1591.52271, 'y3': 789.70367, 'x4': 1490.97314, 'y4': 788.47333}}, {'name': 'book', 'class': 0, 'confidence': 0.74262, 'box': {'x1': 1157.23877, 'y1': 2151.72241, 'x2': 1272.46558, 'y2': 2148.18774, 'x3': 1233.41797, 'y3': 875.24426, 'x4': 1118.19116, 'y4': 878.77881}}, {'name': 'book', 'class': 0, 'confidence': 0.58797, 'box': {'x1': 1902.82422, 'y1': 2198.26636, 'x2': 2002.69678, 'y2': 2199.96509, 'x3': 2028.70679, 'y3': 670.87231, 'x4': 1928.83423, 'y4': 669.17346}}, {'name': 'book', 'class': 0, 'confidence': 0.56269, 'box': {'x1': 1710.62183, 'y1': 3022.11987, 'x2': 1736.32983, 'y2': 2637.97388, 'x3': 371.00467, 'y3': 2546.60327, 'x4': 345.29672, 'y4': 2930.74927}}]\n"
     ]
    }
   ],
   "source": [
    "# Create the output directory, if needed\n",
    "OUTPUT_DIR = get_next_directory(os.path.join(HOME_DIR, \"output/predict\"))\n",
    "os.makedirs(OUTPUT_DIR+\"/book\", exist_ok=True)\n",
    "\n",
    "# Get only filename with no directories and no extension\n",
    "filename = os.path.splitext(os.path.basename(source))[0]\n",
    "\n",
    "# Process results\n",
    "with open(OUTPUT_DIR+\"/results.json\", \"w\") as text_file:\n",
    "    for result in results:\n",
    "        \n",
    "        if (len(result) > 0):\n",
    "            result.show()\n",
    "\n",
    "            print(result.to_json(), file=text_file)\n",
    "\n",
    "            for idx, obb in enumerate(result.obb.xyxyxyxy):\n",
    "                points = obb.cpu().numpy().reshape((-1, 1, 2)).astype(int)\n",
    "                rect = cv2.minAreaRect(points)\n",
    "\n",
    "                # Rotate the image slightly so that it aligns with the axes.\n",
    "                img_cropped = crop_rect(result.orig_img, rect)\n",
    "\n",
    "                # Ensure the image is wider than tall and also return a variant rotated by 180 degrees.\n",
    "                img, img_rotated_180 = prepare_for_ocr(img_cropped)\n",
    "\n",
    "                cv2.imwrite(os.path.join(OUTPUT_DIR, \"book\", f\"{filename}_{idx}.jpg\"), img)\n",
    "                cv2.imwrite(os.path.join(OUTPUT_DIR, \"book\", f\"{filename}_rotated-180_{idx}.jpg\"), img_rotated_180)\n",
    "\n",
    "            result.save_txt(OUTPUT_DIR+\"/results.txt\", save_conf=True)\n",
    "\n",
    "            print(result.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7470ef06",
   "metadata": {},
   "source": [
    "# Perform OCR on each book image found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba21b051",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import pytesseract\n",
    "from pytesseract import Output\n",
    "\n",
    "\n",
    "def detect_text_regions(image, east_model, min_confidence=0.5, nms_threshold=0.8):\n",
    "    \"\"\"\n",
    "    Detects text regions in an image using the EAST text detector with debugging.\n",
    "    \"\"\"\n",
    "    # Load the image and grab its dimensions\n",
    "    (H, W) = image.shape[:2]\n",
    "\n",
    "    # Define the EAST model input dimensions\n",
    "    #newW, newH = 320, 320\n",
    "    # Alternative: Define the new width and height for the image (must be multiples of 32)\n",
    "    newW, newH = (W // 32) * 32, (H // 32) * 32\n",
    "    (rH, rW) = (H / float(newH), W / float(newW))  # Determine scale factors\n",
    "\n",
    "    # Resize the image to fit the EAST model input\n",
    "    resized_image = cv2.resize(image, (newW, newH))\n",
    "    (H, W) = resized_image.shape[:2]\n",
    "\n",
    "    # Define the two output layer names for the EAST detector model that\n",
    "    # we are interested -- the first is the output probabilities and the\n",
    "    # second can be used to derive the bounding box coordinates of text\n",
    "    layerNames = [\n",
    "        \"feature_fusion/Conv_7/Sigmoid\",\n",
    "        \"feature_fusion/concat_3\"]\n",
    "\n",
    "    # Prepare the input blob for the EAST model\n",
    "    blob = cv2.dnn.blobFromImage(resized_image, 1.0, (W, H),\n",
    "                                 (123.68, 116.78, 103.94), swapRB=True, crop=False)\n",
    "\n",
    "\n",
    "    # Perform a forward pass to get scores and geometry\n",
    "    east_model.setInput(blob)\n",
    "    (scores, geometry) = east_model.forward(layerNames)\n",
    "\n",
    "    # Decode predictions\n",
    "    (detections, confidences) = decodeBoundingBoxes(scores, geometry, min_confidence)\n",
    "\n",
    "    # Apply non-maxima suppression to suppress weak, overlapping bounding boxes\n",
    "    indices = cv2.dnn.NMSBoxes(detections, confidences, score_threshold=min_confidence, nms_threshold=nms_threshold)\n",
    "\n",
    "    if isinstance(indices, tuple):  # Check if indices is a tuple (empty result)\n",
    "        indices = np.array([])  # Convert it into an empty NumPy array\n",
    "\n",
    "    # apply morphological operations to merge nearby bounding boxes\n",
    "    kernel = np.ones((10, 10), np.uint8)\n",
    "    mask = np.zeros((H, W), dtype=np.uint8)\n",
    "\n",
    "    # Draw detected boxes onto a mask\n",
    "    for i in indices.flatten():\n",
    "        (startX, startY, endX, endY) = detections[i]\n",
    "        startX = int(startX * rW)\n",
    "        startY = int(startY * rH)\n",
    "        endX = int(endX * rW)\n",
    "        endY = int(endY * rH)\n",
    "        cv2.rectangle(mask, (startX, startY), (endX, endY), 255, -1)\n",
    "\n",
    "    # Dilate the mask to merge nearby detections\n",
    "    mask = cv2.dilate(mask, kernel, iterations=1)\n",
    "\n",
    "    # Find contours from the merged mask\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Collect the new bounding boxes\n",
    "    boxes = []\n",
    "    for contour in contours:\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        boxes.append((x, y, x + w, y + h))\n",
    "\n",
    "    return boxes\n",
    "\n",
    "\n",
    "def decodeBoundingBoxes(scores, geometry, scoreThresh):\n",
    "\n",
    "    # ASSERT dimensions and shapes of geometry and scores #\n",
    "    assert len(scores.shape) == 4, \"Incorrect dimensions of scores\"\n",
    "    assert len(geometry.shape) == 4, \"Incorrect dimensions of geometry\"\n",
    "    assert scores.shape[0] == 1, \"Invalid dimensions of scores\"\n",
    "    assert geometry.shape[0] == 1, \"Invalid dimensions of geometry\"\n",
    "    assert scores.shape[1] == 1, \"Invalid dimensions of scores\"\n",
    "    assert geometry.shape[1] == 5, \"Invalid dimensions of geometry\"\n",
    "    assert scores.shape[2] == geometry.shape[2], \"Invalid dimensions of scores and geometry\"\n",
    "    assert scores.shape[3] == geometry.shape[3], \"Invalid dimensions of scores and geometry\"\n",
    "\n",
    "    detections = []\n",
    "    confidences = []\n",
    "\n",
    "    (numRows, numCols) = scores.shape[2:4]\n",
    "\n",
    "    for y in range(0, numRows):\n",
    "\n",
    "        # Extract data from scores. The geometrical data is used to derive \n",
    "        # potential bounding box coordinates that surround text.\n",
    "        scoresData = scores[0, 0, y]\n",
    "        xData0 = geometry[0, 0, y]\n",
    "        xData1 = geometry[0, 1, y]\n",
    "        xData2 = geometry[0, 2, y]\n",
    "        xData3 = geometry[0, 3, y]\n",
    "        anglesData = geometry[0, 4, y]\n",
    "\n",
    "        for x in range(0, numCols):\n",
    "            score = scoresData[x]\n",
    "\n",
    "            # If score is lower than threshold score, move to next x\n",
    "            if (score < scoreThresh):\n",
    "                continue\n",
    "\n",
    "            # Calculate offset\n",
    "            (offsetX, offsetY) = (x * 4.0, y * 4.0)\n",
    "\n",
    "\n",
    "            # Extract the rotation angle for the prediction and then\n",
    "            # compute the sin and cosine.\n",
    "            angle = anglesData[x]\n",
    "            (cos, sin) = (np.cos(angle), np.sin(angle))\n",
    "\n",
    "            h = xData0[x] + xData2[x]\n",
    "            w = xData1[x] + xData3[x]\n",
    "\n",
    "            # Compute both the starting and ending (x, y)-coordinates for\n",
    "            # the text prediction bounding box.\n",
    "            endX = int(offsetX + (cos * xData1[x]) + (sin * xData2[x]))\n",
    "            endY = int(offsetY - (sin * xData1[x]) + (cos * xData2[x]))\n",
    "            startX = int(endX - w)\n",
    "            startY = int(endY - h)\n",
    "            \n",
    "            # Add the bounding box coordinates and probability score to\n",
    "            # our respective lists.\n",
    "            detections.append((startX, startY, endX, endY))\n",
    "            confidences.append(scoresData[x])\n",
    "\n",
    "    # Return detections and confidences\n",
    "    return (detections, confidences)\n",
    "\n",
    "\n",
    "def showBoundingBoxes(image, boxes):\n",
    "    \"\"\"\n",
    "    Visualizes the bounding boxes on the input image.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if vertices are empty\n",
    "    if boxes is None or len(boxes) == 0:\n",
    "        print(\"No bounding boxes to display.\")\n",
    "        return\n",
    "    \n",
    "    # Show bounding boxes\n",
    "    for i, (startX, startY, endX, endY) in enumerate(boxes):\n",
    "\n",
    "        # Draw the lines\n",
    "        cv2.rectangle(image, (startX, startY), (endX, endY), color=(0, 255, 0), thickness=2)\n",
    "\n",
    "        # Add the index label\n",
    "        text_x, text_y = startX+10, startY+40\n",
    "        cv2.putText(image, f\"{i}\", (text_x, text_y),\n",
    "                    fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=1.2,\n",
    "                    color=(0, 255, 0), thickness=2)\n",
    "\n",
    "    # Display the image with bounding boxes\n",
    "    cv2.imshow(\"Bounding boxes\", image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "def fourPointsTransform(frame, vertices):\n",
    "    vertices = np.asarray(vertices)\n",
    "    outputSize = (100, 32)\n",
    "    targetVertices = np.array([\n",
    "        [0, outputSize[1] - 1],\n",
    "        [0, 0],\n",
    "        [outputSize[0] - 1, 0],\n",
    "        [outputSize[0] - 1, outputSize[1] - 1]], dtype=\"float32\")\n",
    "\n",
    "    rotationMatrix = cv2.getPerspectiveTransform(vertices, targetVertices)\n",
    "    result = cv2.warpPerspective(frame, rotationMatrix, outputSize)\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def rotate_image(image, angle):\n",
    "    \"\"\"\n",
    "    Rotates an image by the specified angle without cropping.\n",
    "\n",
    "    Args:\n",
    "        image (numpy.ndarray): The input image.\n",
    "        angle (float): The angle to rotate.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The rotated image.\n",
    "    \"\"\"\n",
    "    (h, w) = image.shape[:2]\n",
    "    (cx, cy) = (w // 2, h // 2)\n",
    "\n",
    "    # Compute the rotation matrix\n",
    "    M = cv2.getRotationMatrix2D((cx, cy), angle, 1.0)\n",
    "    cos = np.abs(M[0, 0])\n",
    "    sin = np.abs(M[0, 1])\n",
    "\n",
    "    # Compute new bounding dimensions\n",
    "    new_w = int((h * sin) + (w * cos))\n",
    "    new_h = int((h * cos) + (w * sin))\n",
    "\n",
    "    # Adjust rotation matrix to account for translation\n",
    "    M[0, 2] += (new_w / 2) - cx\n",
    "    M[1, 2] += (new_h / 2) - cy\n",
    "\n",
    "    # Perform the rotation\n",
    "    return cv2.warpAffine(image, M, (new_w, new_h))\n",
    "\n",
    "\n",
    "def ocr_onImage(image_path):\n",
    "    \"\"\"\n",
    "    Perform OCR on an image, forcing horizontal text detection.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the image for OCR.\n",
    "\n",
    "    Returns:\n",
    "        str: The OCR-detected text.\n",
    "    \"\"\"\n",
    "\n",
    "    east_model_path = os.path.join(HOME_DIR, \"notebooks\", \"east_text_detection.pb\")\n",
    "\n",
    "    # Open image with Pillow to access DPI metadata\n",
    "    pil_image = Image.open(image_path)\n",
    "    dpi = pil_image.info.get(\"dpi\", (72, 72))  # Default to 72 DPI if not present\n",
    "\n",
    "    # Convert Pillow image to OpenCV format\n",
    "    image = np.array(pil_image)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # --- Detect text regions using EAST, correct orientation, and perform OCR. ---\n",
    "\n",
    "    # Load the pre-trained EAST model\n",
    "    print(\"[INFO] loading EAST text detector...\")\n",
    "    east_model = cv2.dnn.readNet(east_model_path)\n",
    "\n",
    "    # Detect text regions using EAST\n",
    "    boxes = detect_text_regions(image, east_model)\n",
    "\n",
    "    # Visualize all bounding boxes found.\n",
    "    showBoundingBoxes(image.copy(), boxes)\n",
    "\n",
    "    # Load the image\n",
    "    ocr_results = {}\n",
    "\n",
    "                \n",
    "    # # Get cropped image using perspective transform\n",
    "    # cropped_images = []\n",
    "    # cropped_image = fourPointsTransform(image, boxes)\n",
    "    # cropped_image = cv2.cvtColor(cropped_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # cropped_images.append(cropped_image)\n",
    "\n",
    "    # # Process each detected region\n",
    "    # for i, imageWithText in enumerate(cropped_images):\n",
    "\n",
    "    #     cv2.imshow(f\"Region {i}\", imageWithText)\n",
    "    #     cv2.waitKey(0)\n",
    "\n",
    "    #     # Perform OCR on the corrected region\n",
    "    #     ocr_text = pytesseract.image_to_string(imageWithText, config=\"--psm 6\")\n",
    "    #     ocr_results[f\"text_region_{i}\"] = ocr_text.strip()\n",
    "\n",
    "    return ocr_results\n",
    "    \n",
    "\n",
    "# get grayscale image\n",
    "def grayscale(image):\n",
    "    return cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# noise removal\n",
    "def denoise(image):\n",
    "    return cv2.medianBlur(image, 5)\n",
    "\n",
    "# thresholding\n",
    "def thresholding(image):\n",
    "    return cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c5f7f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book 0 found\n",
      "[INFO] loading EAST text detector...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-16 22:12:17.498 python[96051:763322] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-02-16 22:12:17.498 python[96051:763322] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/andreas/Documents/Projekte/Objekterkennung.yolo11/output/predict41/book/books_00005_0.jpg ->\n",
      "[INFO] loading EAST text detector...\n",
      "/Users/andreas/Documents/Projekte/Objekterkennung.yolo11/output/predict41/book/books_00005_rotated-180_0.jpg ->\n",
      "Book 1 found\n",
      "[INFO] loading EAST text detector...\n",
      "/Users/andreas/Documents/Projekte/Objekterkennung.yolo11/output/predict41/book/books_00005_1.jpg ->\n",
      "[INFO] loading EAST text detector...\n",
      "/Users/andreas/Documents/Projekte/Objekterkennung.yolo11/output/predict41/book/books_00005_rotated-180_1.jpg ->\n",
      "Book 2 found\n",
      "[INFO] loading EAST text detector...\n",
      "/Users/andreas/Documents/Projekte/Objekterkennung.yolo11/output/predict41/book/books_00005_2.jpg ->\n",
      "[INFO] loading EAST text detector...\n",
      "/Users/andreas/Documents/Projekte/Objekterkennung.yolo11/output/predict41/book/books_00005_rotated-180_2.jpg ->\n",
      "Book 3 found\n",
      "[INFO] loading EAST text detector...\n",
      "/Users/andreas/Documents/Projekte/Objekterkennung.yolo11/output/predict41/book/books_00005_3.jpg ->\n",
      "[INFO] loading EAST text detector...\n",
      "/Users/andreas/Documents/Projekte/Objekterkennung.yolo11/output/predict41/book/books_00005_rotated-180_3.jpg ->\n",
      "Book 4 found\n",
      "[INFO] loading EAST text detector...\n",
      "/Users/andreas/Documents/Projekte/Objekterkennung.yolo11/output/predict41/book/books_00005_4.jpg ->\n",
      "[INFO] loading EAST text detector...\n",
      "/Users/andreas/Documents/Projekte/Objekterkennung.yolo11/output/predict41/book/books_00005_rotated-180_4.jpg ->\n",
      "Book 5 found\n",
      "[INFO] loading EAST text detector...\n",
      "/Users/andreas/Documents/Projekte/Objekterkennung.yolo11/output/predict41/book/books_00005_5.jpg ->\n",
      "[INFO] loading EAST text detector...\n",
      "/Users/andreas/Documents/Projekte/Objekterkennung.yolo11/output/predict41/book/books_00005_rotated-180_5.jpg ->\n",
      "Book 6 found\n",
      "[INFO] loading EAST text detector...\n",
      "/Users/andreas/Documents/Projekte/Objekterkennung.yolo11/output/predict41/book/books_00005_6.jpg ->\n",
      "[INFO] loading EAST text detector...\n",
      "/Users/andreas/Documents/Projekte/Objekterkennung.yolo11/output/predict41/book/books_00005_rotated-180_6.jpg ->\n",
      "Book 7 found\n",
      "[INFO] loading EAST text detector...\n",
      "/Users/andreas/Documents/Projekte/Objekterkennung.yolo11/output/predict41/book/books_00005_7.jpg ->\n",
      "[INFO] loading EAST text detector...\n",
      "/Users/andreas/Documents/Projekte/Objekterkennung.yolo11/output/predict41/book/books_00005_rotated-180_7.jpg ->\n",
      "Book 8 found\n",
      "[INFO] loading EAST text detector...\n",
      "/Users/andreas/Documents/Projekte/Objekterkennung.yolo11/output/predict41/book/books_00005_8.jpg ->\n",
      "[INFO] loading EAST text detector...\n",
      "/Users/andreas/Documents/Projekte/Objekterkennung.yolo11/output/predict41/book/books_00005_rotated-180_8.jpg ->\n",
      "Book 9 found\n",
      "[INFO] loading EAST text detector...\n",
      "/Users/andreas/Documents/Projekte/Objekterkennung.yolo11/output/predict41/book/books_00005_9.jpg ->\n",
      "[INFO] loading EAST text detector...\n",
      "/Users/andreas/Documents/Projekte/Objekterkennung.yolo11/output/predict41/book/books_00005_rotated-180_9.jpg ->\n",
      "Book 10 found\n",
      "[INFO] loading EAST text detector...\n",
      "/Users/andreas/Documents/Projekte/Objekterkennung.yolo11/output/predict41/book/books_00005_10.jpg ->\n",
      "[INFO] loading EAST text detector...\n",
      "/Users/andreas/Documents/Projekte/Objekterkennung.yolo11/output/predict41/book/books_00005_rotated-180_10.jpg ->\n",
      "Book 11 found\n",
      "[INFO] loading EAST text detector...\n",
      "No bounding boxes to display.\n",
      "/Users/andreas/Documents/Projekte/Objekterkennung.yolo11/output/predict41/book/books_00005_11.jpg ->\n",
      "[INFO] loading EAST text detector...\n",
      "No bounding boxes to display.\n",
      "/Users/andreas/Documents/Projekte/Objekterkennung.yolo11/output/predict41/book/books_00005_rotated-180_11.jpg ->\n",
      "Book 12 found\n",
      "[INFO] loading EAST text detector...\n",
      "/Users/andreas/Documents/Projekte/Objekterkennung.yolo11/output/predict41/book/books_00005_12.jpg ->\n",
      "[INFO] loading EAST text detector...\n",
      "/Users/andreas/Documents/Projekte/Objekterkennung.yolo11/output/predict41/book/books_00005_rotated-180_12.jpg ->\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mDer Kernel ist beim Ausführen von Code in der aktuellen Zelle oder einer vorherigen Zelle abgestürzt. \n",
      "\u001b[1;31mBitte überprüfen Sie den Code in der/den Zelle(n), um eine mögliche Fehlerursache zu identifizieren. \n",
      "\u001b[1;31mKlicken Sie <a href='https://aka.ms/vscodeJupyterKernelCrash'>hier</a>, um weitere Informationen zu erhalten. \n",
      "\u001b[1;31mWeitere Informationen finden Sie unter Jupyter <a href='command:jupyter.viewOutput'>Protokoll</a>."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "\n",
    "#import easyocr\n",
    "import pytesseract\n",
    "\n",
    "# --- Needs tesseract on the path. I've installed it via homebrew.\n",
    "\n",
    "# Get only filename with no directories and no extension\n",
    "filename = os.path.splitext(os.path.basename(source))[0]\n",
    "\n",
    "for result in results:\n",
    "\n",
    "    if (len(result) > 0):\n",
    "        # for object detection and instance separation\n",
    "        # i=1\n",
    "\n",
    "        # for OBB\n",
    "        i=0\n",
    "\n",
    "        for detection in result.summary(): \n",
    "            if (detection['name'] == 'book'):\n",
    "                print(f\"Book {i} found\")\n",
    "\n",
    "                # for object detection and instance separation\n",
    "                # image_filename = f\"image{i}.jpg\" if i>1 else 'image.jpg'\n",
    "                # image_path = OUTPUT_DIR + '/book/' + image_filename\n",
    "                \n",
    "                # for OBB\n",
    "                # Perform OCR on all (both) image variants.\n",
    "                image_variants = [\n",
    "                    f\"{filename}_{i}.jpg\",  # Original image\n",
    "                    f\"{filename}_rotated-180_{i}.jpg\"  # 180-degree rotated image\n",
    "                ]\n",
    "\n",
    "                # Iterate over each variant, process the OCR, and print the result\n",
    "                for variant_filename in image_variants:\n",
    "                    img_path = os.path.join(OUTPUT_DIR, \"book\", variant_filename)\n",
    "                    detected_texts = ocr_onImage(img_path)\n",
    "\n",
    "                    # Display OCR results\n",
    "                    print(f\"{img_path} ->\")\n",
    "                    for region, text in detected_texts.items():\n",
    "                        print(f\"    {region}: {text}\")\n",
    "\n",
    "                i += 1\n",
    "            else:\n",
    "                print(\"Skipping\", detection['name'], '...')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
