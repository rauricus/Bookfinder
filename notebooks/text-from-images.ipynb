{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bbdfed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "NOTEBOOKS_DIR = os.getcwd()\n",
    "HOME_DIR = os.path.dirname(NOTEBOOKS_DIR)\n",
    "\n",
    "def get_next_directory(base_path=\"output/predict\"):\n",
    "    # Check if the base directory exists\n",
    "    if not os.path.exists(base_path):\n",
    "        return base_path\n",
    "    else:\n",
    "        # Find the next available numbered directory\n",
    "        i = 2\n",
    "        while os.path.exists(f\"{base_path}{i}\"):\n",
    "            i += 1\n",
    "        return f\"{base_path}{i}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f738e69-8ca5-46a7-8a8c-abeff29fa2ce",
   "metadata": {},
   "source": [
    "# Perform object detection\n",
    "\n",
    "Perform the inference with the loaded model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81faa6e6-cf3a-433e-a847-229f1c4bf69f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /Users/andreas/Documents/Projekte/Objekterkennung.yolo11/example-files/books/books_00005.png: 480x640 228.6ms\n",
      "Speed: 2.6ms preprocess, 228.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "#model = YOLO(\"yolo11s.pt\", )  # load an official model\n",
    "# model = YOLO(\"yolo11s-seg.pt\")  # load an official model (instance segmentation)\n",
    "model = YOLO(HOME_DIR+\"/runs/obb/train/weights/best.pt\")  # load an official model (Oriented Bounding Boxes Object Detection)\n",
    "#model = YOLO(HOME_DIR+\"/runs/segment/train/weights/best.pt\")  # load my custom model\n",
    "\n",
    "# source = 'https://ultralytics.com/images/bus.jpg'\n",
    "# source = HOME_DIR+'/example-files/IMG_3688.png'\n",
    "# source = HOME_DIR+'/example-files/books'\n",
    "# source = HOME_DIR+'/example-files/books.mov'\n",
    "source = HOME_DIR+'/example-files/books/books_00005.png'\n",
    "\n",
    "# Predict with the model\n",
    "results = model.predict(source, conf=0.5)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153d223f",
   "metadata": {},
   "source": [
    "# Adjust images (OBB only)\n",
    "\n",
    "From: https://github.com/ultralytics/ultralytics/issues/9344\n",
    "\n",
    "Switching to an OBB (oriented bounding box) model means you'll be working with rotated bounding boxes. The .boxes attribute for an OBB model will contain the center coordinates, width, height, and angle in radians.\n",
    "\n",
    "I took the following crop_rect function from here, as it also rotates the image: https://github.com/ultralytics/ultralytics/issues/13650"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa95859c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def crop_rect(img, rect, interpolation=cv2.INTER_CUBIC):\n",
    "    \"\"\"\n",
    "    Extracts and rectifies a rotated bounding box from an image.\n",
    "\n",
    "    This function takes an image and an oriented bounding box (OBB), rotates the \n",
    "    image such that the bounding box becomes axis-aligned (rectangular), and then \n",
    "    crops the bounding box area.\n",
    "\n",
    "    Args:\n",
    "        img (numpy.ndarray): The input image from which the bounding box is extracted.\n",
    "        rect (tuple): The oriented bounding box parameters.\n",
    "                     - rect[0]: Center coordinates of the bounding box (x, y).\n",
    "                     - rect[1]: Size of the bounding box (width, height).\n",
    "                     - rect[2]: Rotation angle of the bounding box (in degrees).\n",
    "        interpolation (int, optional): Interpolation method used when rotating the image.\n",
    "                                       Defaults to cv2.INTER_CUBIC.\n",
    "\n",
    "    Returns:\n",
    "        cropped_image (numpy.ndarray): The cropped rectangle region from the rotated image.\n",
    "    \"\"\"\n",
    "\n",
    "    # Process:\n",
    "    #    1. Extracts the center, size, and angle of the bounding box.\n",
    "    #    2. Computes a rotation matrix to align the bounding box with the image axes.\n",
    "    #    3. Rotates the image based on the calculated rotation matrix.\n",
    "    #    4. Crops the now axis-aligned bounding box from the rotated image.\n",
    "\n",
    "\n",
    "    # get the parameter of the small rectangle\n",
    "    center, size, angle = rect[0], rect[1], rect[2]\n",
    "    center, size = tuple(map(int, center)), tuple(map(int, size))\n",
    "\n",
    "    # get row and col num in img\n",
    "    height, width = img.shape[0], img.shape[1]\n",
    "\n",
    "    # calculate the rotation matrix\n",
    "    M = cv2.getRotationMatrix2D(center, angle, 1)\n",
    "    # rotate the original image\n",
    "    img_rot = cv2.warpAffine(img, M, (width, height), flags=interpolation)\n",
    "\n",
    "    # now rotated rectangle becomes vertical, and we crop it\n",
    "    img_crop = cv2.getRectSubPix(img_rot, size, center)\n",
    "\n",
    "    return img_crop\n",
    "\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def prepare_for_ocr(img):\n",
    "    \"\"\"\n",
    "    Processes a cropped rectangle for OCR detection by ensuring the image is wider than tall\n",
    "    and generating both the original (or rotated) and a 180-degree rotated variant.\n",
    "    \n",
    "    Args:\n",
    "        img (numpy.ndarray): The cropped rectangle image.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (processed_img, rotated_180_img)\n",
    "               - processed_img: Image oriented to be wider than tall.\n",
    "               - rotated_180_img: 180-degree rotated version of `processed_img`.\n",
    "    \"\"\"\n",
    "    # Get image dimensions\n",
    "    height, width = img.shape[:2]\n",
    "\n",
    "    # Rotate 90 degrees clockwise if the image is taller than it is wide\n",
    "    if height > width:\n",
    "        img = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\n",
    "\n",
    "    # Generate the 180-degree rotated image\n",
    "    rotated_180_img = cv2.rotate(img, cv2.ROTATE_180)\n",
    "\n",
    "    return img, rotated_180_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bff58f",
   "metadata": {},
   "source": [
    "# Save results, rotate and crop images (OBB only)\n",
    "\n",
    "The main code is again from https://github.com/ultralytics/ultralytics/issues/13650"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a9f9c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'book', 'class': 0, 'confidence': 0.92829, 'box': {'x1': 2138.47437, 'y1': 2193.13525, 'x2': 2474.677, 'y2': 2211.39307, 'x3': 2559.83325, 'y3': 643.29034, 'x4': 2223.63062, 'y4': 625.03278}}, {'name': 'book', 'class': 0, 'confidence': 0.9155, 'box': {'x1': 2761.04761, 'y1': 2158.79468, 'x2': 2935.44067, 'y2': 2165.02563, 'x3': 2985.83765, 'y3': 754.52094, 'x4': 2811.44458, 'y4': 748.28986}}, {'name': 'book', 'class': 0, 'confidence': 0.91296, 'box': {'x1': 2932.27344, 'y1': 2170.69312, 'x2': 3104.52686, 'y2': 2176.56909, 'x3': 3153.72607, 'y3': 734.31604, 'x4': 2981.47266, 'y4': 728.43994}}, {'name': 'book', 'class': 0, 'confidence': 0.90544, 'box': {'x1': 2501.48193, 'y1': 2174.44922, 'x2': 2753.79492, 'y2': 2187.65234, 'x3': 2831.42773, 'y3': 704.10999, 'x4': 2579.11475, 'y4': 690.90662}}, {'name': 'book', 'class': 0, 'confidence': 0.8734, 'box': {'x1': 3395.00635, 'y1': 2191.7168, 'x2': 3565.3833, 'y2': 2203.33594, 'x3': 3671.73682, 'y3': 643.80695, 'x4': 3501.35986, 'y4': 632.18793}}, {'name': 'book', 'class': 0, 'confidence': 0.84513, 'box': {'x1': 1702.9198, 'y1': 2186.4397, 'x2': 1848.88708, 'y2': 2191.30103, 'x3': 1898.17175, 'y3': 711.41461, 'x4': 1752.20447, 'y4': 706.55353}}, {'name': 'book', 'class': 0, 'confidence': 0.82285, 'box': {'x1': 1574.646, 'y1': 2192.63843, 'x2': 1693.94653, 'y2': 2194.91626, 'x3': 1723.32422, 'y3': 656.21381, 'x4': 1604.02368, 'y4': 653.9361}}, {'name': 'book', 'class': 0, 'confidence': 0.80939, 'box': {'x1': 3053.93799, 'y1': 2170.57959, 'x2': 3401.18311, 'y2': 2199.17871, 'x3': 3529.78809, 'y3': 637.69336, 'x4': 3182.54297, 'y4': 609.09412}}, {'name': 'book', 'class': 0, 'confidence': 0.79645, 'box': {'x1': 1333.94604, 'y1': 2134.27075, 'x2': 1490.21387, 'y2': 2132.92896, 'x3': 1479.5564, 'y3': 891.95294, 'x4': 1323.28857, 'y4': 893.29498}}, {'name': 'book', 'class': 0, 'confidence': 0.77945, 'box': {'x1': 1473.78955, 'y1': 2192.73584, 'x2': 1574.33911, 'y2': 2193.96631, 'x3': 1591.52271, 'y3': 789.70367, 'x4': 1490.97314, 'y4': 788.47333}}, {'name': 'book', 'class': 0, 'confidence': 0.74262, 'box': {'x1': 1157.23877, 'y1': 2151.72241, 'x2': 1272.46558, 'y2': 2148.18774, 'x3': 1233.41797, 'y3': 875.24426, 'x4': 1118.19116, 'y4': 878.77881}}, {'name': 'book', 'class': 0, 'confidence': 0.58797, 'box': {'x1': 1902.82422, 'y1': 2198.26636, 'x2': 2002.69678, 'y2': 2199.96509, 'x3': 2028.70679, 'y3': 670.87231, 'x4': 1928.83423, 'y4': 669.17346}}, {'name': 'book', 'class': 0, 'confidence': 0.56269, 'box': {'x1': 1710.62183, 'y1': 3022.11987, 'x2': 1736.32983, 'y2': 2637.97388, 'x3': 371.00467, 'y3': 2546.60327, 'x4': 345.29672, 'y4': 2930.74927}}]\n"
     ]
    }
   ],
   "source": [
    "# Create the output directory, if needed\n",
    "OUTPUT_DIR = get_next_directory(os.path.join(HOME_DIR, \"output/predict\"))\n",
    "os.makedirs(OUTPUT_DIR+\"/book\", exist_ok=True)\n",
    "\n",
    "# Get only filename with no directories and no extension\n",
    "filename = os.path.splitext(os.path.basename(source))[0]\n",
    "\n",
    "# Process results\n",
    "with open(OUTPUT_DIR+\"/results.json\", \"w\") as text_file:\n",
    "    for result in results:\n",
    "        \n",
    "        if (len(result) > 0):\n",
    "            result.show()\n",
    "\n",
    "            print(result.to_json(), file=text_file)\n",
    "\n",
    "            for idx, obb in enumerate(result.obb.xyxyxyxy):\n",
    "                points = obb.cpu().numpy().reshape((-1, 1, 2)).astype(int)\n",
    "                rect = cv2.minAreaRect(points)\n",
    "\n",
    "                # Rotate the image slightly so that it aligns with the axes.\n",
    "                img_cropped = crop_rect(result.orig_img, rect)\n",
    "\n",
    "                # Ensure the image is wider than tall and also return a variant rotated by 180 degrees.\n",
    "                img, img_rotated_180 = prepare_for_ocr(img_cropped)\n",
    "\n",
    "                cv2.imwrite(os.path.join(OUTPUT_DIR, \"book\", f\"{filename}_{idx}.jpg\"), img)\n",
    "                cv2.imwrite(os.path.join(OUTPUT_DIR, \"book\", f\"{filename}_rotated-180_{idx}.jpg\"), img_rotated_180)\n",
    "\n",
    "            result.save_txt(OUTPUT_DIR+\"/results.txt\", save_conf=True)\n",
    "\n",
    "            print(result.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7470ef06",
   "metadata": {},
   "source": [
    "# Perform OCR on each book image found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba21b051",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import pytesseract\n",
    "from pytesseract import Output\n",
    "\n",
    "\n",
    "def detect_text_regions(image, east_model_path, min_confidence=0.5, nms_threshold=0.4):\n",
    "    \"\"\"\n",
    "    Detects text regions in an image using the EAST text detector with debugging.\n",
    "    \"\"\"\n",
    "    # Load the image and grab its dimensions\n",
    "    (H, W) = image.shape[:2]\n",
    "\n",
    "    # Define the EAST model input dimensions\n",
    "    newW, newH = 320, 320\n",
    "\n",
    "    # Resize the image to fit the EAST model input\n",
    "    resized_image = cv2.resize(image, (newW, newH))\n",
    "    (rH, rW) = (H / float(newH), W / float(newW))  # Scale factors\n",
    "\n",
    "    # Load the pre-trained EAST model\n",
    "    net = cv2.dnn.readNet(east_model_path)\n",
    "\n",
    "    # Prepare the input blob for the EAST model\n",
    "    blob = cv2.dnn.blobFromImage(resized_image, 1.0, (newW, newH),\n",
    "                                 (123.68, 116.78, 103.94), swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "\n",
    "    # Perform a forward pass to get scores and geometry\n",
    "    (scores, geometry) = net.forward([\"feature_fusion/Conv_7/Sigmoid\",\n",
    "                                      \"feature_fusion/concat_3\"])\n",
    "\n",
    "    # Decode predictions\n",
    "    [detections, confidences] = decodeBoundingBoxes(scores, geometry, min_confidence)\n",
    "\n",
    "    # Apply non-maxima suppression to eliminate overlapping boxes\n",
    "    indices = cv2.dnn.NMSBoxesRotated(detections, confidences, min_confidence, nms_threshold)\n",
    "\n",
    "    vertices = []  # Collect all bounding boxes\n",
    "    if len(indices) > 0:\n",
    "        for i in indices.flatten():\n",
    "            box = cv2.boxPoints(detections[i])  # Get 4 corners of the bounding box\n",
    "\n",
    "            # Scale the bounding box coordinates\n",
    "            for j in range(4):\n",
    "                box[j][0] *= rW\n",
    "                box[j][1] *= rH\n",
    "\n",
    "            vertices.append(box)  # Append instead of overwrite\n",
    "\n",
    "    return vertices\n",
    "\n",
    "\n",
    "def decodeBoundingBoxes(scores, geometry, scoreThresh):\n",
    "    detections = []\n",
    "    confidences = []\n",
    "\n",
    "    ############ CHECK DIMENSIONS AND SHAPES OF geometry AND scores ############\n",
    "    assert len(scores.shape) == 4, \"Incorrect dimensions of scores\"\n",
    "    assert len(geometry.shape) == 4, \"Incorrect dimensions of geometry\"\n",
    "    assert scores.shape[0] == 1, \"Invalid dimensions of scores\"\n",
    "    assert geometry.shape[0] == 1, \"Invalid dimensions of geometry\"\n",
    "    assert scores.shape[1] == 1, \"Invalid dimensions of scores\"\n",
    "    assert geometry.shape[1] == 5, \"Invalid dimensions of geometry\"\n",
    "    assert scores.shape[2] == geometry.shape[2], \"Invalid dimensions of scores and geometry\"\n",
    "    assert scores.shape[3] == geometry.shape[3], \"Invalid dimensions of scores and geometry\"\n",
    "\n",
    "    height, width = scores.shape[2], scores.shape[3]\n",
    "    for y in range(0, height):\n",
    "\n",
    "        # Extract data from scores\n",
    "        scoresData = scores[0][0][y]\n",
    "        x0_data, x1_data, x2_data, x3_data = geometry[0][:4, y]\n",
    "        anglesData = geometry[0][4, y]\n",
    "\n",
    "        for x in range(0, width):\n",
    "            score = scoresData[x]\n",
    "\n",
    "            # If score is lower than threshold score, move to next x\n",
    "            if (score < scoreThresh):\n",
    "                continue\n",
    "\n",
    "            # Calculate offset\n",
    "            offsetX, offsetY = x * 4.0, y * 4.0\n",
    "            angle = anglesData[x]\n",
    "\n",
    "            # Calculate cos and sin of angle\n",
    "            cosA, sinA = np.cos(angle), np.sin(angle)\n",
    "\n",
    "            h = x0_data[x] + x2_data[x]\n",
    "            w = x1_data[x] + x3_data[x]\n",
    "\n",
    "            # Calculate offset\n",
    "            offset = [offsetX + cosA * x1_data[x] + sinA * x2_data[x],\n",
    "                      offsetY - sinA * x1_data[x] + cosA * x2_data[x]]\n",
    "\n",
    "            # Find points for rectangle\n",
    "            p1 = (-sinA * h + offset[0], -cosA * h + offset[1])\n",
    "            p3 = (-cosA * w + offset[0], sinA * w + offset[1])\n",
    "            center = (0.5 * (p1[0] + p3[0]), 0.5 * (p1[1] + p3[1]))\n",
    "            \n",
    "            detections.append((center, (w, h), -1 * angle * 180.0 / np.pi))\n",
    "            confidences.append(float(score))\n",
    "\n",
    "    # Return detections and confidences\n",
    "    return [detections, confidences]\n",
    "\n",
    "\n",
    "def debug_visualize_bounding_boxes(image, vertices):\n",
    "    \"\"\"\n",
    "    Visualizes the bounding boxes on the input image.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if vertices are empty\n",
    "    if vertices is None or len(vertices) == 0:\n",
    "        print(\"No bounding boxes to display.\")\n",
    "        return\n",
    "    \n",
    "    # Convert vertices to a compatible format for cv2.polylines\n",
    "    vertices_as_int = [np.array(vertex, dtype=np.int32).reshape(-1, 1, 2) for vertex in vertices]\n",
    "\n",
    "    # Draw all bounding boxes at once\n",
    "    cv2.polylines(image, vertices_as_int, isClosed=True, color=(0, 255, 0), thickness=2)\n",
    "\n",
    "    # Add index labels to each bounding box\n",
    "    for i, vertex in enumerate(vertices):\n",
    "\n",
    "        vertex_array = np.array(vertex).reshape(-1, 2)  # Ensure it's a 2D array with shape (4, 2)\n",
    "\n",
    "        # Calculate the center of the bounding box\n",
    "        center = np.mean(vertex_array, axis=0).astype(int)\n",
    "        center_x, center_y = center[0], center[1]  # Unpack the coordinates\n",
    "\n",
    "        # Add the index label\n",
    "        cv2.putText(image, f\"{i}\", (center_x, center_y),\n",
    "                    fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=1.5,\n",
    "                    color=(0, 255, 0), thickness=1)\n",
    "\n",
    "    # Display the image with bounding boxes\n",
    "    cv2.imshow(\"Visualized bounding boxes\", image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "def fourPointsTransform(frame, vertices):\n",
    "    vertices = np.asarray(vertices)\n",
    "    outputSize = (100, 32)\n",
    "    targetVertices = np.array([\n",
    "        [0, outputSize[1] - 1],\n",
    "        [0, 0],\n",
    "        [outputSize[0] - 1, 0],\n",
    "        [outputSize[0] - 1, outputSize[1] - 1]], dtype=\"float32\")\n",
    "\n",
    "    rotationMatrix = cv2.getPerspectiveTransform(vertices, targetVertices)\n",
    "    result = cv2.warpPerspective(frame, rotationMatrix, outputSize)\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def rotate_image(image, angle):\n",
    "    \"\"\"\n",
    "    Rotates an image by the specified angle without cropping.\n",
    "\n",
    "    Args:\n",
    "        image (numpy.ndarray): The input image.\n",
    "        angle (float): The angle to rotate.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The rotated image.\n",
    "    \"\"\"\n",
    "    (h, w) = image.shape[:2]\n",
    "    (cx, cy) = (w // 2, h // 2)\n",
    "\n",
    "    # Compute the rotation matrix\n",
    "    M = cv2.getRotationMatrix2D((cx, cy), angle, 1.0)\n",
    "    cos = np.abs(M[0, 0])\n",
    "    sin = np.abs(M[0, 1])\n",
    "\n",
    "    # Compute new bounding dimensions\n",
    "    new_w = int((h * sin) + (w * cos))\n",
    "    new_h = int((h * cos) + (w * sin))\n",
    "\n",
    "    # Adjust rotation matrix to account for translation\n",
    "    M[0, 2] += (new_w / 2) - cx\n",
    "    M[1, 2] += (new_h / 2) - cy\n",
    "\n",
    "    # Perform the rotation\n",
    "    return cv2.warpAffine(image, M, (new_w, new_h))\n",
    "\n",
    "\n",
    "def ocr_onImage(image_path):\n",
    "    \"\"\"\n",
    "    Perform OCR on an image, forcing horizontal text detection.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the image for OCR.\n",
    "\n",
    "    Returns:\n",
    "        str: The OCR-detected text.\n",
    "    \"\"\"\n",
    "\n",
    "    east_model_path = os.path.join(HOME_DIR, \"notebooks\", \"east_text_detection.pb\")\n",
    "\n",
    "    # Open image with Pillow to access DPI metadata\n",
    "    pil_image = Image.open(image_path)\n",
    "    dpi = pil_image.info.get(\"dpi\", (72, 72))  # Default to 72 DPI if not present\n",
    "\n",
    "    # Convert Pillow image to OpenCV format\n",
    "    image = np.array(pil_image)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # --- Detect text regions using EAST, correct orientation, and perform OCR. ---\n",
    "\n",
    "    # Detect text regions using EAST\n",
    "    vertices = detect_text_regions(image, east_model_path)\n",
    "\n",
    "    debug_visualize_bounding_boxes(image.copy(), vertices)\n",
    "\n",
    "    # Load the image\n",
    "    ocr_results = {}\n",
    "\n",
    "                \n",
    "    # Get cropped image using perspective transform\n",
    "    cropped_images = []\n",
    "    cropped_image = fourPointsTransform(image, vertices)\n",
    "    cropped_image = cv2.cvtColor(cropped_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    cropped_images.append(cropped_image)\n",
    "\n",
    "    # Process each detected region\n",
    "    for i, imageWithText in enumerate(cropped_images):\n",
    "\n",
    "        cv2.imshow(f\"Region {i}\", imageWithText)\n",
    "        cv2.waitKey(0)\n",
    "\n",
    "        # Perform OCR on the corrected region\n",
    "        ocr_text = pytesseract.image_to_string(imageWithText, config=\"--psm 6\")\n",
    "        ocr_results[f\"text_region_{i}\"] = ocr_text.strip()\n",
    "\n",
    "    return ocr_results\n",
    "    \n",
    "\n",
    "# get grayscale image\n",
    "def grayscale(image):\n",
    "    return cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# noise removal\n",
    "def denoise(image):\n",
    "    return cv2.medianBlur(image, 5)\n",
    "\n",
    "# thresholding\n",
    "def thresholding(image):\n",
    "    return cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87c5f7f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book 0 found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-12 11:41:01.509 python[30772:236509] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-02-12 11:41:01.509 python[30772:236509] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.11.0) /Users/runner/miniforge3/conda-bld/libopencv_1739279513328/work/modules/imgproc/src/imgwarp.cpp:3623: error: (-215:Assertion failed) src.checkVector(2, CV_32F) == 4 && dst.checkVector(2, CV_32F) == 4 in function 'getPerspectiveTransform'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m variant_filename \u001b[38;5;129;01min\u001b[39;00m image_variants:\n\u001b[1;32m     38\u001b[0m     img_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(OUTPUT_DIR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbook\u001b[39m\u001b[38;5;124m\"\u001b[39m, variant_filename)\n\u001b[0;32m---> 39\u001b[0m     detected_texts \u001b[38;5;241m=\u001b[39m \u001b[43mocr_onImage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# Display OCR results\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ->\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 228\u001b[0m, in \u001b[0;36mocr_onImage\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;66;03m# Get cropped image using perspective transform\u001b[39;00m\n\u001b[1;32m    227\u001b[0m cropped_images \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 228\u001b[0m cropped_image \u001b[38;5;241m=\u001b[39m \u001b[43mfourPointsTransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    229\u001b[0m cropped_image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(cropped_image, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2GRAY)\n\u001b[1;32m    231\u001b[0m cropped_images\u001b[38;5;241m.\u001b[39mappend(cropped_image)\n",
      "Cell \u001b[0;32mIn[5], line 157\u001b[0m, in \u001b[0;36mfourPointsTransform\u001b[0;34m(frame, vertices)\u001b[0m\n\u001b[1;32m    150\u001b[0m outputSize \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m32\u001b[39m)\n\u001b[1;32m    151\u001b[0m targetVertices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\n\u001b[1;32m    152\u001b[0m     [\u001b[38;5;241m0\u001b[39m, outputSize[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m    153\u001b[0m     [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    154\u001b[0m     [outputSize[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    155\u001b[0m     [outputSize[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, outputSize[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]], dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 157\u001b[0m rotationMatrix \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetPerspectiveTransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvertices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargetVertices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m result \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mwarpPerspective(frame, rotationMatrix, outputSize)\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.11.0) /Users/runner/miniforge3/conda-bld/libopencv_1739279513328/work/modules/imgproc/src/imgwarp.cpp:3623: error: (-215:Assertion failed) src.checkVector(2, CV_32F) == 4 && dst.checkVector(2, CV_32F) == 4 in function 'getPerspectiveTransform'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "\n",
    "#import easyocr\n",
    "import pytesseract\n",
    "\n",
    "# --- Needs tesseract on the path. I've installed it via homebrew.\n",
    "\n",
    "# Get only filename with no directories and no extension\n",
    "filename = os.path.splitext(os.path.basename(source))[0]\n",
    "\n",
    "for result in results:\n",
    "\n",
    "    if (len(result) > 0):\n",
    "        # for object detection and instance separation\n",
    "        # i=1\n",
    "\n",
    "        # for OBB\n",
    "        i=0\n",
    "\n",
    "        for detection in result.summary(): \n",
    "            if (detection['name'] == 'book'):\n",
    "                print(f\"Book {i} found\")\n",
    "\n",
    "                # for object detection and instance separation\n",
    "                # image_filename = f\"image{i}.jpg\" if i>1 else 'image.jpg'\n",
    "                # image_path = OUTPUT_DIR + '/book/' + image_filename\n",
    "                \n",
    "                # for OBB\n",
    "                # Perform OCR on all (both) image variants.\n",
    "                image_variants = [\n",
    "                    f\"{filename}_{i}.jpg\",  # Original image\n",
    "                    f\"{filename}_rotated-180_{i}.jpg\"  # 180-degree rotated image\n",
    "                ]\n",
    "\n",
    "                # Iterate over each variant, process the OCR, and print the result\n",
    "                for variant_filename in image_variants:\n",
    "                    img_path = os.path.join(OUTPUT_DIR, \"book\", variant_filename)\n",
    "                    detected_texts = ocr_onImage(img_path)\n",
    "\n",
    "                    # Display OCR results\n",
    "                    print(f\"{img_path} ->\")\n",
    "                    for region, text in detected_texts.items():\n",
    "                        print(f\"    {region}: {text}\")\n",
    "\n",
    "                i += 1\n",
    "            else:\n",
    "                print(\"Skipping\", detection['name'], '...')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (yolo11)",
   "language": "python",
   "name": "yolo11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
